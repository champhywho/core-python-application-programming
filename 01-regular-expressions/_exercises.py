#!/usr/bin/env python3

# Regular Expressions.

# Create regular expressions that:
# 1. Recognize the following strings: 'bat', 'bit', 'but', 'hat',
# 'hit', or 'hut'.
patt = r'[bh][aiu]t'

# 2. Match any pair of words separated by a single space, that is,
# first and last names.
patt = r'[A-Za-z]+ [A-Za-z]+'

# 3. Match any word and single letter separated by a comma and single space,
# as in last name, first inital.
patt = r'[A-Za-z]+, [A-Za-z]'

# 4. Match the set of all valid Python identifiers.
patt = r'[A-Za-z_][\w_]*'

# 5. Match a street address according to your local format (keep your regex
# general enough to match any number of street words, including the type
# designation). For example: "prosp. Geroiv Stalingradu, 247B", or
# "prosp. 50-richcha Peremogy, 36/7", or "vul. 50-richcha Peremogy, 7-D"
patt = r'([A-Za-z\.]+) ([\w -]+), ([\d/]+\-?[A-Za-z]?)'

# 6. Match simple Web domain names that begin with "www." and end with a ".com"
# suffix; for example, www.yahoo.com. Extra Credit: If your regex also supports
# other high-level domain names, such as .edu, .net, etc. (for example,
# www.foothill.edu).
patt = r'www\.[\w\-]+\.(com|edu|net)'

# 7. Match the set of the string representations of all Python integers.
patt = r'\d+'

# 8. Match the set of the string representations of all Python longs.
patt = r'\d+[lL]'

# 9. Match the set of the string representations of all Python floats.
patt = r'(\d*)\.((?(1)\d*|\d+))'

# 10. Match the set of the string representations of all Python complex numbers.
patt = r'\(?(\-?[\d\.]+)?([\-\+]?[\d\.]+[jJ])\)?'

# 11. Match the set of all valid e-mail addresses (start with a loose regex,
# and then try to tighten it as much as you can, yet maintain correct
# functionality).
patt = r'(\w+(?:[\-\.]\w+)*)@((?:\w+[\-\.])*\w+).(com|edu|net|org)'

# 12. Match the set of all valid Web site addresses (URLs) (start with a loose
# regex, and then try to tighten it as much as you can, yet maintain correct
# functionality).
patt = r'(https?)://((?:\w+[\-\.])*\w+).(com|edu|net|org)/(.*)'

# 13. Create a regex that would extract the actual type name from the string.
# Your function should take a string like this <class 'int'> and return int.
# (Ditto for all other types, such as 'float', 'builtin_function_or_method',
# etc.) Note: You are implementing the value that is stored in the __name__
# attribute for classes and some built-in types.
patt = r'<class \'(\w+)\'>'

# 14. Processing Dates. The regex pattern that matched the single or
# double-digit string representations of the months January to September
# (0?[1-9]). Create the regex that represents the remaining three months
# in the standard calendar.
patt = r'(0?[1-9]|1[0-2])'

# 15. Processing Credit Card Numbers. We gave you the regex pattern that
# matched credit card (CC) numbers ([0-9]{15,16}). However, this pattern does
# not allow for hyphens separating blocks of numbers. Create the regex that
# allows hyphens, but only in the correct locations. For example, 15-digit CC
# numbers have a pattern of 4-6-5, indicating four digits-hyphen-six
# digits-hyphen-five digits; and 16-digit CC numbers have a 4-4-4-4 pattern.
# Remember to "ballon" the size of the entire string correctly. Extra Credit:
# There is a standard algorithm for determining whether a CC number is valid.
# Write some code that not only recognizes a correctly formatted CC number,
# but also a valid one.
patt = r'^(\d{4})-(\d{4}|(\d{6}))-((?(3)\d{5}|\d{4}))((?(3)|-\d{4}))$'



# Playing with gendata.py. The next set of Exercises (16-27) deal specifically
# with the data that is generated by gendata.py.

# 16. Update the code for gendata.py so that the data is written directly to
# redata.txt rather than output to the screen.

# Done

# 17. Determine how many times each day of the week shows up for any
# incarnation of redata.txt. (Alternatively, you can also count how many times
# eachmonth of the year was chosen.)
from collections import defaultdict

weekdays = collections.defaultdict(lambda: 0)
months = collections.defaultdict(lambda: 0)
with open(r'redata.txt') as f:
    for line in f:
        weekdays[line[:3]] += 1
        months[line[3:6]] += 1

print(weekdays.items())
print(months.items())

# 18. Ensure that there is no data corruption in redata.txt by confirming that
# the first integer of the integert field matches the timestamp given at the
# beginning of each output line.
from time import ctime

with open(r'redata.txt') as f:
    for line in f:
        line = line.split('::')
        intgr = int(line[-1].split('-')[0])
        if line[0] != ctime(intgr):
            print("Do not match!")


# Create Regular Expressions That:
# 19. Extract the complete timestamps from each line.
patt = r'(.+?)::'
with open(r'redata.txt') as f:
    [re.match(patt, line).group(1) for line in f]

# 20. Extract the complete e-mail address from each line.
patt = r'.+::(.+)::.+'
with open(r'redata.txt') as f:
    [re.match(patt, line).group(1) for line in f]

# 21. Extract only the months from the timestamps.
patt = r'[A-Za-z]{3} ([A-Za-z]{3})'
with open(r'redata.txt') as f:
    [re.match(patt, line).group(1) for line in f]

# 22. Extract only the years from the timestamps.
patt = r'(\d{4}(?=::))'
with open(r'redata.txt') as f:
    [re.search(patt, line).group(1) for line in f]

# 23. Extract only the time (HH:MM:SS) from the timestamps.
patt = r'(\d{2}:\d{2}:\d{2})'
with open(r'redata.txt') as f:
    [re.search(patt, line).group(1) for line in f]

# 24. Extract only the login and domain names (both the main domain name and
# the hight-level domain together) from the e-mail address.
patt = r'::(\w+)@([\w\.]+)::'
with open(fl) as f:
    [re.search(patt, line).groups() for line in f]

# 25. Extract only the login and domain names (both the main domain name and
# the hight-level domain) from the e-mail address.
patt = r'::(\w+)@([\w\.]+)\.(\w+)::'
with open(fl) as f:
    [re.search(patt, line).groups() for line in f]

# 26. Replace the e-mail address from each line of data with your e-mail
# address.
patt = r'::(.+)::'
myem = '::my@email.com::'
with open(fl) as f:
    [re.sub(patt, myem, line) for line in f]

# 27. Extract the months, days, and years from the timestamps and output them
# in 'Mon, Day, Year' format, iterating over each line only once.
patt = r'([A-Za-z]{3}) {1,2}(\d{1,2}) (?:\d{2}:\d{2}:\d{2}) (\d{4}(?=::))'
with open(fl) as f:
	for line in f:
		dt = re.findall(patt, line)[0]
		print("{0}, {1}, {2}".format(dt[0], dt[1], dt[2]))


# Processing Telephone Numbers.
# 28. Area codes (the first set of three-digits and the accompanying hyphen)
# are optional, that is, your regex should match both 800-555-1212 as well as
# just 555-1212.
patt = r'^(\d{3}-)?\d{3}-\d{4}$'

# 29. Either parenthesized or hyphendated area codes are supported, not to
# mention optionl; make your regex match 800-555-1212, 555-1212 and also
# (800) 555-1212.
patt = r'^((\d{3}-)|(\(\d{3}\) ))?\d{3}-\d{4}$'


# Regex Unilities. The final set of exercises make useful utility scrips when
# processing online data.
# 30. HTML Generation. Given a list of links (and optional short description),
# whether user-provided on command-line, via input from another scrip, or from
# a database, generate a Web page (.html) that includes all links as hypertext
# anchors, which upon viewing in a Web browser, allows users to click those
# links and visit the corresponding site. If the short descriptin is provided,
# use that as the hypertext instead of the URL.

list_of_links = [
    ('https://developer.mozilla.org/en-US/', 'Mozilla Developer Network'),
    ('https://developer.chrome.com/devtools', ''), # Chrome DevTools Overview
    ('http://www.ibm.com/developerworks/learn/', 'IBM developerWorks')
]

with open('links_web_page.html', 'w') as page:
    page.write('<!DOCTYPE html>\n<html>\n<body>\n')
    anchor = '<a href="{0}">{1}</a><br />\n'
    for link in list_of_links:
        page.write(anchor.format(link[0], link[1] if link[1] else link[0]))
    page.write('</body>\n</html>')

# 31. Tweet Scrub. Sometimes all you want to see is the plain text of a tweet
# as posted to the Twetter service by user. Create a function that takes a tweet
# and an optional 'meta' flag defaulted False, and then returns a string of the
# scrubbed tweet, removing all the extraneous information, such as an "RT"
# notation for "retweet", a leading ., and all "#hashtags". If the meta flag is
# True, then also return a dict containing the the metadata. This can include
# a key "RT", whose value is a tuple of strings of users who retweeted the
# message, and/or a key "hashtags" with a tuple of the hashtags. If the values
# don't exist (empty tuples), then don't even bother creating a key-value entry
# for them.


# 32. Amazon Screenscraper. Create a script that helps your to keep track of
# your favorite books and how they're doing on Amazon (or any other online
# bookseller that tracks book rankings). For example, the Amazon link for any
# book is of the format, http://amazon.com/db/ISBN (for example,
# http://amazon.com/dp/0132678209). You can then change the domain name to
# check out equivalent rankings on Amazon sites in other countries, such as
# Germany (.de), France (.fr), Japan (.jp), China (.cn), and the UK (.co.uk).
# Use regular expressions or a markup parser, such as BeautifulSoup, lxml, or
# html5lib to parse the ranking, and then let the user pass in a command-line
# argument that specifies whether the output should be in plain text, perhaps
# for inclusion in an e-mail body, or formatted in HTML for Web consumption.
book_list = {
    '0132678209': (
        'Wesley Chun',
        'Core Python Applications Programming',
        '3rd Edition'),
    '1491946008': (
        'Luciano Ramalho',
        'Fluent Python,
        '1st Edition'),
    '1491910291': (
        'Ryan Mitchell',
        'Web Scraping with Python: Collecting Data from the Modern Web',
        '1st Edition'),
    '1449364829': (
        'Harry J. W. Percival',
        'Test-Driven Development with Python',
        '1st Edition'),
    '1783555130': (
        'Sebastian Raschka',
        'Python Machine Learning',
        '1st Edition'),
}

import re
from urllib.request import urlopen
from urllib.error import HTTPError

def book_rankings(isbn, plain=True):
    hldoms = ('.com', '.de', '.fr', '.co.jp', '.cn', '.co.uk')
    patt = r'<span class="a-icon-alt">([\d\.]+) [\w\s]+</span>'
    site = r'http://www.amazon{0}/dp/{1}'

    for dom in hldoms:
        url = site.format(dom, isbn)
        try:
            html = urlopen(url).read().decode()
            try:
                rank = re.search(patt, html)
                if plain:
                    rank = rank.group(1)
                else:
                    rank = rank.group()
            except AttributeError:
                rank = None
        except HTTPError as err:
            rank = 'Error_{0}'.format(err.getcode())
        print('amazon{0:10}{1}'.format(dom, rank))
